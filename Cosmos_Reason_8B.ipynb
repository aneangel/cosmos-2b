{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7934d1",
   "metadata": {},
   "source": [
    "# Physical AI-Driven Action Sequence Analysis and MDP Modeling Using NVIDIA Cosmos Reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f48ea2",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Video Recording\n",
    "\n",
    "1) Only use one hand and manipulate one object at a time.\n",
    "2) Do not try to do the same order as the other people! Every person can pick a\n",
    "different sequence of actions to complete the task.\n",
    "3) Try to make your hand gestures obvious when grasping or releasing an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124 -q\n",
    "%pip install transformers accelerate -q\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything-2.git -q\n",
    "%pip install opencv-python supervision -q\n",
    "%pip install matplotlib numpy Pillow tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ef68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Import Libraries\n",
    "# ============================================================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor, build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8193aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "VIDEO_DIR = \"./demonstrations\"\n",
    "OUTPUT_DIR = \"./demonstrations/objects_tracked\"\n",
    "FRAMES_DIR = os.path.join(OUTPUT_DIR, \"frames\")  # Temporary frames for SAM2 video\n",
    "OUTPUT_VIDEOS_DIR = os.path.join(OUTPUT_DIR, \"videos\")\n",
    "TRACKING_DATA_DIR = os.path.join(OUTPUT_DIR, \"tracking_data\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_VIDEOS_DIR, exist_ok=True)\n",
    "os.makedirs(TRACKING_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Gather all video files\n",
    "video_files = sorted([f for f in os.listdir(VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "print(f\"Found {len(video_files)} videos:\")\n",
    "for v in video_files:\n",
    "    print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Grounding DINO for Initial Detection\n",
    "# ============================================================\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "GDINO_MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "gdino_processor = AutoProcessor.from_pretrained(GDINO_MODEL_ID)\n",
    "gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained(GDINO_MODEL_ID).to(DEVICE)\n",
    "gdino_model.eval()\n",
    "print(\"Grounding DINO loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load SAM 2 for Video Segmentation & Tracking\n",
    "# ============================================================\n",
    "\n",
    "# SAM2 checkpoint (you already have this)\n",
    "SAM2_CHECKPOINT = \"sam2_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "# Download if not present\n",
    "if not os.path.exists(SAM2_CHECKPOINT):\n",
    "    print(\"Downloading SAM2 checkpoint...\")\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\",\n",
    "        SAM2_CHECKPOINT\n",
    "    )\n",
    "\n",
    "# Build video predictor for tracking\n",
    "sam2_video_predictor = build_sam2_video_predictor(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE)\n",
    "\n",
    "# Build image predictor for initial segmentation\n",
    "sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE)\n",
    "sam2_image_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "print(\"SAM 2 Video Predictor loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workspace/Tabletop Detection\n",
    "# ============================================================\n",
    "\n",
    "def detect_tabletop_region(image, method=\"color\"):\n",
    "    \"\"\"\n",
    "    Detect the tabletop/workspace region to filter out background objects.\n",
    "    \n",
    "    Methods:\n",
    "    - \"color\": Detect table by dominant color (works for colored tables)\n",
    "    - \"lower_half\": Simple heuristic - table is typically in lower portion\n",
    "    - \"full\": No filtering, use entire frame\n",
    "    \n",
    "    Returns: mask where True = workspace area\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    if method == \"lower_half\":\n",
    "        # Simple heuristic: table is in lower 70% of frame\n",
    "        mask = np.zeros((h, w), dtype=bool)\n",
    "        mask[int(h * 0.2):, :] = True\n",
    "        return mask\n",
    "    \n",
    "    elif method == \"color\":\n",
    "        # Detect table surface by color - typically a solid color\n",
    "        # Convert to HSV for better color segmentation\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Detect dominant non-white/non-black regions in lower half\n",
    "        lower_region = hsv[int(h * 0.5):, :]\n",
    "        \n",
    "        # Calculate histogram of hue values\n",
    "        hist = cv2.calcHist([lower_region], [0], None, [180], [0, 180])\n",
    "        dominant_hue = np.argmax(hist)\n",
    "        \n",
    "        # Create mask for table color (with tolerance)\n",
    "        lower_bound = np.array([max(0, dominant_hue - 15), 30, 50])\n",
    "        upper_bound = np.array([min(180, dominant_hue + 15), 255, 255])\n",
    "        color_mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "        \n",
    "        # Combine with spatial prior (lower portion more likely to be table)\n",
    "        spatial_weight = np.linspace(0.3, 1.0, h).reshape(-1, 1)\n",
    "        spatial_mask = np.tile(spatial_weight, (1, w))\n",
    "        \n",
    "        combined = (color_mask > 0).astype(float) * spatial_mask\n",
    "        mask = combined > 0.5\n",
    "        \n",
    "        # Clean up with morphology\n",
    "        kernel = np.ones((20, 20), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return mask.astype(bool)\n",
    "    \n",
    "    else:  # \"full\"\n",
    "        return np.ones((h, w), dtype=bool)\n",
    "\n",
    "\n",
    "def is_in_workspace(box, workspace_mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Check if a detected object is within the workspace region.\n",
    "    \n",
    "    Args:\n",
    "        box: [x1, y1, x2, y2] bounding box\n",
    "        workspace_mask: Boolean mask of workspace region\n",
    "        threshold: Minimum overlap ratio required\n",
    "    \n",
    "    Returns: True if object is in workspace\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(workspace_mask.shape[1], x2), min(workspace_mask.shape[0], y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return False\n",
    "    \n",
    "    box_region = workspace_mask[y1:y2, x1:x2]\n",
    "    overlap_ratio = np.mean(box_region)\n",
    "    \n",
    "    return overlap_ratio >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dice Detection with Grounding DINO\n",
    "# ============================================================\n",
    "\n",
    "def normalize_dice_label(raw_label):\n",
    "    \"\"\"\n",
    "    Normalize Grounding DINO labels to canonical dice colors.\n",
    "    \"\"\"\n",
    "    raw = raw_label.lower().strip()\n",
    "    \n",
    "    color_order = [\"green\", \"red\", \"blue\"]\n",
    "    for color in color_order:\n",
    "        if color in raw:\n",
    "            return f\"{color}_dice\"\n",
    "    \n",
    "    if \"dice\" in raw or \"cube\" in raw:\n",
    "        return \"unknown_dice\"\n",
    "    \n",
    "    return raw_label\n",
    "\n",
    "\n",
    "def classify_dice_by_color(image, box):\n",
    "    \"\"\"\n",
    "    Classify a dice by analyzing the dominant color in its bounding box.\n",
    "    More reliable than relying solely on Grounding DINO labels.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    h, w = image.shape[:2]\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(w, x2), min(h, y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return \"unknown_dice\"\n",
    "    \n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Define color ranges in HSV\n",
    "    color_ranges = {\n",
    "        \"red_dice\": [\n",
    "            (np.array([0, 100, 100]), np.array([10, 255, 255])),      # Red lower\n",
    "            (np.array([160, 100, 100]), np.array([180, 255, 255]))    # Red upper\n",
    "        ],\n",
    "        \"green_dice\": [\n",
    "            (np.array([35, 80, 80]), np.array([85, 255, 255]))        # Green\n",
    "        ],\n",
    "        \"blue_dice\": [\n",
    "            (np.array([90, 80, 80]), np.array([130, 255, 255]))       # Blue\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    color_scores = {}\n",
    "    for color_name, ranges in color_ranges.items():\n",
    "        total_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)\n",
    "        for lower, upper in ranges:\n",
    "            mask = cv2.inRange(hsv, lower, upper)\n",
    "            total_mask = cv2.bitwise_or(total_mask, mask)\n",
    "        color_scores[color_name] = np.sum(total_mask) / (total_mask.size * 255)\n",
    "    \n",
    "    # Return color with highest score if above threshold\n",
    "    best_color = max(color_scores, key=color_scores.get)\n",
    "    if color_scores[best_color] > 0.15:  # At least 15% of pixels match\n",
    "        return best_color\n",
    "    \n",
    "    return \"unknown_dice\"\n",
    "\n",
    "\n",
    "def detect_colored_dice(image, workspace_mask=None, box_threshold=0.25, text_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detect all colored dice in an image using Grounding DINO.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        workspace_mask: Optional mask to filter detections\n",
    "        box_threshold: Confidence threshold for boxes\n",
    "        text_threshold: Confidence threshold for text matching\n",
    "    \n",
    "    Returns: List of dicts with {box, label, score, color}\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pil_image = Image.fromarray(image)\n",
    "        np_image = image\n",
    "    else:\n",
    "        pil_image = image\n",
    "        np_image = np.array(image)\n",
    "    \n",
    "    # Text prompt for dice detection\n",
    "    text_prompt = \"green dice . red dice . blue dice . green cube . red cube . blue cube .\"\n",
    "    \n",
    "    inputs = gdino_processor(\n",
    "        images=pil_image,\n",
    "        text=text_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gdino_model(**inputs)\n",
    "    \n",
    "    results = gdino_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]\n",
    "    )[0]\n",
    "    \n",
    "    boxes = results[\"boxes\"].cpu().numpy()\n",
    "    scores = results[\"scores\"].cpu().numpy()\n",
    "    labels = results[\"labels\"]\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = scores >= box_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = [labels[i] for i in range(len(labels)) if mask[i]]\n",
    "    \n",
    "    detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Filter by workspace if provided\n",
    "        if workspace_mask is not None and not is_in_workspace(box, workspace_mask):\n",
    "            continue\n",
    "        \n",
    "        # Classify color based on actual pixel values\n",
    "        color = classify_dice_by_color(np_image, box)\n",
    "        \n",
    "        detections.append({\n",
    "            \"box\": box,\n",
    "            \"score\": float(score),\n",
    "            \"label\": normalize_dice_label(label),\n",
    "            \"color\": color\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def apply_nms(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply non-maximum suppression to remove duplicate detections.\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return detections\n",
    "    \n",
    "    boxes = np.array([d[\"box\"] for d in detections])\n",
    "    scores = np.array([d[\"score\"] for d in detections])\n",
    "    \n",
    "    # Sort by score\n",
    "    order = scores.argsort()[::-1]\n",
    "    \n",
    "    keep = []\n",
    "    while len(order) > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        if len(order) == 1:\n",
    "            break\n",
    "        \n",
    "        # Compute IoU with remaining boxes\n",
    "        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])\n",
    "        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])\n",
    "        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])\n",
    "        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])\n",
    "        \n",
    "        inter = np.maximum(0, xx2 - xx1) * np.maximum(0, yy2 - yy1)\n",
    "        area_i = (boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1])\n",
    "        areas = (boxes[order[1:], 2] - boxes[order[1:], 0]) * (boxes[order[1:], 3] - boxes[order[1:], 1])\n",
    "        iou = inter / (area_i + areas - inter + 1e-6)\n",
    "        \n",
    "        remaining = np.where(iou < iou_threshold)[0] + 1\n",
    "        order = order[remaining]\n",
    "    \n",
    "    return [detections[i] for i in keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b106f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Video Frame Extraction for SAM2\n",
    "# ============================================================\n",
    "\n",
    "def extract_frames_for_sam2(video_path, output_dir, sample_rate=1):\n",
    "    \"\"\"\n",
    "    Extract frames from video for SAM2 video predictor.\n",
    "    SAM2 requires JPEG frames in a directory.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_dir: Directory to save frames\n",
    "        sample_rate: Extract every Nth frame (1 = all frames)\n",
    "    \n",
    "    Returns: (frame_paths, fps, total_frames, frame_indices)\n",
    "    \"\"\"\n",
    "    # Clear output directory\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frame_paths = []\n",
    "    frame_indices = []\n",
    "    frame_idx = 0\n",
    "    saved_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_idx % sample_rate == 0:\n",
    "            # SAM2 expects sequential naming\n",
    "            frame_path = os.path.join(output_dir, f\"{saved_idx:06d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frame_paths.append(frame_path)\n",
    "            frame_indices.append(frame_idx)\n",
    "            saved_idx += 1\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return frame_paths, fps, total_frames, frame_indices, (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAM2 Video Tracking Core\n",
    "# ============================================================\n",
    "\n",
    "# Color mapping for visualization\n",
    "DICE_COLORS = {\n",
    "    \"green_dice\": (0, 255, 0),      # Green\n",
    "    \"red_dice\": (255, 0, 0),        # Red  \n",
    "    \"blue_dice\": (0, 0, 255),       # Blue\n",
    "    \"unknown_dice\": (255, 255, 0),  # Yellow\n",
    "}\n",
    "\n",
    "\n",
    "def track_dice_in_video(video_path, sample_rate=2, workspace_method=\"lower_half\"):\n",
    "    \"\"\"\n",
    "    Track all colored dice cubes in a video using SAM2.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        sample_rate: Process every Nth frame (lower = more accurate, slower)\n",
    "        workspace_method: Method to detect tabletop (\"lower_half\", \"color\", \"full\")\n",
    "    \n",
    "    Returns: Dictionary with tracking results\n",
    "    \"\"\"\n",
    "    video_name = Path(video_path).stem\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {video_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create temporary frame directory for this video\n",
    "    video_frames_dir = os.path.join(FRAMES_DIR, video_name)\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    print(\"Step 1: Extracting frames...\")\n",
    "    frame_paths, fps, total_frames, frame_indices, (width, height) = \\\n",
    "        extract_frames_for_sam2(video_path, video_frames_dir, sample_rate)\n",
    "    print(f\"  - Extracted {len(frame_paths)} frames (original: {total_frames} @ {fps:.1f} FPS)\")\n",
    "    \n",
    "    # Step 2: Detect dice on first frame\n",
    "    print(\"Step 2: Detecting dice on first frame...\")\n",
    "    first_frame = cv2.cvtColor(cv2.imread(frame_paths[0]), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect workspace/tabletop\n",
    "    workspace_mask = detect_tabletop_region(first_frame, method=workspace_method)\n",
    "    \n",
    "    # Detect dice\n",
    "    detections = detect_colored_dice(first_frame, workspace_mask, box_threshold=0.20)\n",
    "    detections = apply_nms(detections, iou_threshold=0.4)\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        print(\" No dice detected on first frame! Trying without workspace filter...\")\n",
    "        detections = detect_colored_dice(first_frame, None, box_threshold=0.15)\n",
    "        detections = apply_nms(detections, iou_threshold=0.4)\n",
    "    \n",
    "    print(f\"  - Found {len(detections)} dice:\")\n",
    "    for i, det in enumerate(detections):\n",
    "        print(f\"    [{i}] {det['color']} (conf: {det['score']:.2f})\")\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        print(\" No dice found, skipping video\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Initialize SAM2 video predictor\n",
    "    print(\"Step 3: Initializing SAM2 video tracking...\")\n",
    "    \n",
    "    with torch.inference_mode(), torch.autocast(DEVICE, dtype=torch.bfloat16):\n",
    "        state = sam2_video_predictor.init_state(video_path=video_frames_dir)\n",
    "        \n",
    "        # Add each detected dice as a tracking object\n",
    "        object_ids = []\n",
    "        object_colors = {}\n",
    "        \n",
    "        for i, det in enumerate(detections):\n",
    "            obj_id = i + 1  # SAM2 uses 1-indexed object IDs\n",
    "            box = det[\"box\"]\n",
    "            \n",
    "            # Add object with bounding box prompt\n",
    "            _, out_obj_ids, out_mask_logits = sam2_video_predictor.add_new_points_or_box(\n",
    "                inference_state=state,\n",
    "                frame_idx=0,\n",
    "                obj_id=obj_id,\n",
    "                box=box\n",
    "            )\n",
    "            \n",
    "            object_ids.append(obj_id)\n",
    "            object_colors[obj_id] = det[\"color\"]\n",
    "        \n",
    "        print(f\"  - Initialized {len(object_ids)} objects for tracking\")\n",
    "        \n",
    "        # Step 4: Propagate through video\n",
    "        print(\"Step 4: Propagating masks through video...\")\n",
    "        \n",
    "        # Collect all frame masks\n",
    "        video_segments = {}  # {frame_idx: {obj_id: mask}}\n",
    "        \n",
    "        for frame_idx, obj_ids, mask_logits in sam2_video_predictor.propagate_in_video(state):\n",
    "            masks = (mask_logits > 0.0).cpu().numpy()\n",
    "            video_segments[frame_idx] = {}\n",
    "            \n",
    "            for i, obj_id in enumerate(obj_ids):\n",
    "                video_segments[frame_idx][obj_id] = masks[i, 0]  # [H, W] boolean mask\n",
    "        \n",
    "        print(f\"  - Tracked across {len(video_segments)} frames\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"video_name\": video_name,\n",
    "        \"video_path\": video_path,\n",
    "        \"fps\": fps,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"processed_frames\": len(frame_paths),\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"frame_size\": (width, height),\n",
    "        \"frame_indices\": frame_indices,\n",
    "        \"objects\": {\n",
    "            obj_id: {\n",
    "                \"color\": object_colors[obj_id],\n",
    "                \"initial_box\": detections[obj_id - 1][\"box\"].tolist()\n",
    "            }\n",
    "            for obj_id in object_ids\n",
    "        },\n",
    "        \"segments\": video_segments,\n",
    "        \"frame_paths\": frame_paths\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization & Video Output\n",
    "# ============================================================\n",
    "\n",
    "def create_tracking_video(results, output_path):\n",
    "    \"\"\"\n",
    "    Create output video with segmentation masks overlaid.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    frame_paths = results[\"frame_paths\"]\n",
    "    segments = results[\"segments\"]\n",
    "    objects = results[\"objects\"]\n",
    "    fps = results[\"fps\"] / results[\"sample_rate\"]  # Adjust for sampled frames\n",
    "    \n",
    "    # Get frame size\n",
    "    first_frame = cv2.imread(frame_paths[0])\n",
    "    height, width = first_frame.shape[:2]\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Creating output video: {output_path}\")\n",
    "    \n",
    "    for frame_idx, frame_path in enumerate(tqdm(frame_paths, desc=\"  Rendering\")):\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        if frame_idx in segments:\n",
    "            for obj_id, mask in segments[frame_idx].items():\n",
    "                if obj_id in objects:\n",
    "                    color_name = objects[obj_id][\"color\"]\n",
    "                    color_bgr = DICE_COLORS.get(color_name, (255, 255, 0))\n",
    "                    # Convert RGB to BGR for OpenCV\n",
    "                    color_bgr = (color_bgr[2], color_bgr[1], color_bgr[0])\n",
    "                    \n",
    "                    # Apply mask overlay\n",
    "                    mask_3ch = np.stack([mask] * 3, axis=-1)\n",
    "                    overlay = frame.copy()\n",
    "                    overlay[mask] = color_bgr\n",
    "                    frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "                    \n",
    "                    # Draw contour\n",
    "                    contours, _ = cv2.findContours(\n",
    "                        mask.astype(np.uint8), \n",
    "                        cv2.RETR_EXTERNAL, \n",
    "                        cv2.CHAIN_APPROX_SIMPLE\n",
    "                    )\n",
    "                    cv2.drawContours(frame, contours, -1, color_bgr, 2)\n",
    "                    \n",
    "                    # Add label\n",
    "                    if contours:\n",
    "                        M = cv2.moments(contours[0])\n",
    "                        if M[\"m00\"] > 0:\n",
    "                            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                            label = color_name.replace(\"_\", \" \").title()\n",
    "                            cv2.putText(frame, label, (cx - 30, cy - 10),\n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\" Saved: {output_path}\")\n",
    "\n",
    "\n",
    "def visualize_first_frame_detections(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the initial detections on the first frame.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    frame_path = results[\"frame_paths\"][0]\n",
    "    frame = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
    "    segments = results[\"segments\"].get(0, {})\n",
    "    objects = results[\"objects\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Original with boxes\n",
    "    ax1 = axes[0]\n",
    "    ax1.imshow(frame)\n",
    "    ax1.set_title(f\"Initial Detections - {results['video_name']}\")\n",
    "    \n",
    "    for obj_id, obj_info in objects.items():\n",
    "        box = obj_info[\"initial_box\"]\n",
    "        color = DICE_COLORS.get(obj_info[\"color\"], (255, 255, 0))\n",
    "        # Normalize to 0-1 for matplotlib\n",
    "        color_norm = tuple(c / 255 for c in color)\n",
    "        \n",
    "        rect = plt.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            fill=False, edgecolor=color_norm, linewidth=2\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(box[0], box[1] - 5, obj_info[\"color\"].replace(\"_\", \" \"),\n",
    "                color=color_norm, fontsize=10, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # With segmentation masks\n",
    "    ax2 = axes[1]\n",
    "    mask_overlay = frame.copy().astype(float)\n",
    "    \n",
    "    for obj_id, mask in segments.items():\n",
    "        if obj_id in objects:\n",
    "            color = DICE_COLORS.get(objects[obj_id][\"color\"], (255, 255, 0))\n",
    "            for c in range(3):\n",
    "                mask_overlay[:, :, c] = np.where(\n",
    "                    mask, \n",
    "                    mask_overlay[:, :, c] * 0.5 + color[c] * 0.5,\n",
    "                    mask_overlay[:, :, c]\n",
    "                )\n",
    "    \n",
    "    ax2.imshow(mask_overlay.astype(np.uint8))\n",
    "    ax2.set_title(f\"Segmentation Masks - {results['video_name']}\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save Tracking Data\n",
    "# ============================================================\n",
    "\n",
    "def save_tracking_data(results, output_dir):\n",
    "    \"\"\"\n",
    "    Save tracking data (centroids, bounding boxes per frame) to JSON.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    video_name = results[\"video_name\"]\n",
    "    \n",
    "    # Extract centroid and bbox data per frame\n",
    "    tracking_data = {\n",
    "        \"video_name\": video_name,\n",
    "        \"fps\": results[\"fps\"],\n",
    "        \"total_frames\": results[\"total_frames\"],\n",
    "        \"sample_rate\": results[\"sample_rate\"],\n",
    "        \"objects\": {},\n",
    "        \"frames\": {}\n",
    "    }\n",
    "    \n",
    "    # Object info\n",
    "    for obj_id, obj_info in results[\"objects\"].items():\n",
    "        tracking_data[\"objects\"][str(obj_id)] = {\n",
    "            \"color\": obj_info[\"color\"]\n",
    "        }\n",
    "    \n",
    "    # Per-frame tracking\n",
    "    for frame_idx, masks in results[\"segments\"].items():\n",
    "        original_frame_idx = results[\"frame_indices\"][frame_idx]\n",
    "        tracking_data[\"frames\"][str(original_frame_idx)] = {}\n",
    "        \n",
    "        for obj_id, mask in masks.items():\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "            \n",
    "            # Get bounding box from mask\n",
    "            ys, xs = np.where(mask)\n",
    "            if len(xs) == 0:\n",
    "                continue\n",
    "            \n",
    "            bbox = [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
    "            centroid = [int(np.mean(xs)), int(np.mean(ys))]\n",
    "            area = int(np.sum(mask))\n",
    "            \n",
    "            tracking_data[\"frames\"][str(original_frame_idx)][str(obj_id)] = {\n",
    "                \"bbox\": bbox,\n",
    "                \"centroid\": centroid,\n",
    "                \"area\": area,\n",
    "                \"color\": results[\"objects\"][obj_id][\"color\"]\n",
    "            }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_path = os.path.join(output_dir, f\"{video_name}_tracking.json\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(tracking_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved tracking data: {output_path}\")\n",
    "    return tracking_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1aa7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Process All Videos\n",
    "# ============================================================\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 2  # Process every 2nd frame (balance speed/accuracy)\n",
    "WORKSPACE_METHOD = \"lower_half\"  # Options: \"lower_half\", \"color\", \"full\"\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(VIDEO_DIR, video_file)\n",
    "    video_name = Path(video_file).stem\n",
    "    \n",
    "    try:\n",
    "        # Track dice in video\n",
    "        results = track_dice_in_video(\n",
    "            video_path, \n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            workspace_method=WORKSPACE_METHOD\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            all_results[video_name] = results\n",
    "            \n",
    "            # Visualize first frame detections\n",
    "            vis_path = os.path.join(OUTPUT_DIR, f\"{video_name}_detections.png\")\n",
    "            visualize_first_frame_detections(results, save_path=vis_path)\n",
    "            \n",
    "            # Create output video with tracking\n",
    "            output_video_path = os.path.join(OUTPUT_VIDEOS_DIR, f\"{video_name}_tracked.mp4\")\n",
    "            create_tracking_video(results, output_video_path)\n",
    "            \n",
    "            # Save tracking data\n",
    "            save_tracking_data(results, TRACKING_DATA_DIR)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Videos processed: {len(all_results)}/{len(video_files)}\")\n",
    "print(f\"Output videos: {OUTPUT_VIDEOS_DIR}\")\n",
    "print(f\"Tracking data: {TRACKING_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary Statistics\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRACKING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for video_name, results in all_results.items():\n",
    "    objects = results[\"objects\"]\n",
    "    \n",
    "    # Count by color\n",
    "    color_counts = {\"green_dice\": 0, \"red_dice\": 0, \"blue_dice\": 0, \"unknown_dice\": 0}\n",
    "    for obj_info in objects.values():\n",
    "        color = obj_info[\"color\"]\n",
    "        if color in color_counts:\n",
    "            color_counts[color] += 1\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"video\": video_name,\n",
    "        \"green\": color_counts[\"green_dice\"],\n",
    "        \"red\": color_counts[\"red_dice\"],\n",
    "        \"blue\": color_counts[\"blue_dice\"],\n",
    "        \"total\": len(objects),\n",
    "        \"frames\": results[\"processed_frames\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{video_name}:\")\n",
    "    print(f\"  ðŸŸ¢ Green dice: {color_counts['green_dice']}\")\n",
    "    print(f\"  ðŸ”´ Red dice:   {color_counts['red_dice']}\")\n",
    "    print(f\"  ðŸ”µ Blue dice:  {color_counts['blue_dice']}\")\n",
    "    print(f\" Total tracked: {len(objects)} objects over {results['processed_frames']} frames\")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Video':<20} {'Green':>8} {'Red':>8} {'Blue':>8} {'Total':>8} {'Frames':>8}\")\n",
    "print(\"-\"*70)\n",
    "for row in summary_data:\n",
    "    print(f\"{row['video']:<20} {row['green']:>8} {row['red']:>8} {row['blue']:>8} {row['total']:>8} {row['frames']:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99867440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test on Single Video (run this first to verify setup)\n",
    "# ============================================================\n",
    "\n",
    "# Pick the first video for testing\n",
    "TEST_VIDEO = video_files[0] if video_files else None\n",
    "\n",
    "if TEST_VIDEO:\n",
    "    test_video_path = os.path.join(VIDEO_DIR, TEST_VIDEO)\n",
    "    \n",
    "    # Track with higher sample rate for faster testing\n",
    "    test_results = track_dice_in_video(\n",
    "        test_video_path,\n",
    "        sample_rate=5,  # Faster for testing\n",
    "        workspace_method=\"lower_half\"\n",
    "    )\n",
    "    \n",
    "    if test_results:\n",
    "        # Visualize\n",
    "        visualize_first_frame_detections(test_results)\n",
    "        \n",
    "        # Create short test video\n",
    "        test_output = os.path.join(OUTPUT_VIDEOS_DIR, f\"{Path(TEST_VIDEO).stem}_test.mp4\")\n",
    "        create_tracking_video(test_results, test_output)\n",
    "        \n",
    "        print(\"\\nTest complete! Check the output above to verify dice detection.\")\n",
    "        print(\"  If dice are detected correctly, run the 'Process All Videos' cell.\")\n",
    "else:\n",
    "    print(\"No videos found in demonstrations folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af796e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Object and Human Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a769115",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries and Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1991d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchvision\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*video metadata.*\")\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5199a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame_count(video_path, max_frames=500):\n",
    "    \"\"\"\n",
    "    Get the total number of frames in a given video using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        Video_path: path to video\n",
    "\n",
    "    Returns:\n",
    "        int: total number of frames in given video\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: could not open video {video_path}\")\n",
    "        return 60\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "\n",
    "    cap.release()\n",
    "    capped_count = min(frame_count -1, max_frames)\n",
    "\n",
    "    print(f\" Video: {Path(video_path).name} | Frames: {frame_count} | FPS: {fps:.2f} | Duration: {duration:.2f} secs\")\n",
    "    return capped_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e2d79",
   "metadata": {},
   "source": [
    "## Setup Model Function\n",
    "\n",
    "Load the Cosmos Reason model and processor from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63243d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_name=\"nvidia/Cosmos-Reason2-8B\"):\n",
    "    \"\"\"\n",
    "    Load the Cosmos Reason model and processor\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model on Hugging Face\n",
    "        \n",
    "    Returns:\n",
    "        model, processor tuple\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"This may take a few minutes on first run...\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load the processor\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93318fd8",
   "metadata": {},
   "source": [
    "## Analyze Video Function\n",
    "\n",
    "Process a video file with a question, handle system prompts for reasoning, and parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e9abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, question, model, processor, nframes, enable_reasoning=True):\n",
    "    \"\"\"\n",
    "    Analyze a video using Cosmos Reason\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        question: Question to ask about the video\n",
    "        model: The loaded model\n",
    "        processor: The loaded processor\n",
    "        nframes: Number of frames to sample (default: 60, recommended)\n",
    "        enable_reasoning: Whether to enable chain-of-thought reasoning\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'reasoning' and 'answer' keys\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing video: {video_path}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Number of frames: {nframes}\")\n",
    "    print(f\"Reasoning enabled: {enable_reasoning}\")\n",
    "    \n",
    "    # Prepare the system prompt (with reasoning format if enabled)\n",
    "    if enable_reasoning:\n",
    "        system_prompt = \"\"\"Answer the question in the following format:\n",
    "<think>\n",
    "your reasoning\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "your answer\n",
    "</answer>\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"You are a helpful assistant that analyzes videos.\"\n",
    "    \n",
    "    # Prepare the conversation messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": str(video_path),\n",
    "                    \"nframes\": nframes  # Explicit frame count to avoid metadata warning\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process the video and prepare inputs\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    print(\"\\nGenerating response...\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4096  # Recommended to avoid truncation\n",
    "        )\n",
    "    \n",
    "    # Trim the input tokens from the generated output\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] \n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the response\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    # Parse reasoning and answer if reasoning was enabled\n",
    "    if enable_reasoning:\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "        \n",
    "        if \"<think>\" in output_text and \"</think>\" in output_text:\n",
    "            reasoning = output_text.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
    "        \n",
    "        if \"<answer>\" in output_text and \"</answer>\" in output_text:\n",
    "            answer = output_text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "        elif \"</think>\" in output_text:\n",
    "            # Sometimes the answer comes after </think> without tags\n",
    "            answer = output_text.split(\"</think>\")[1].strip()\n",
    "        else:\n",
    "            answer = output_text\n",
    "            \n",
    "        return {\n",
    "            \"reasoning\": reasoning,\n",
    "            \"answer\": answer,\n",
    "            \"full_output\": output_text\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"reasoning\": \"\",\n",
    "            \"answer\": output_text,\n",
    "            \"full_output\": output_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2f9df",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Instantiate the model and processor using the Cosmos Reason 2B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66292b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: nvidia/Cosmos-Reason2-8B\n",
      "This may take a few minutes on first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 41221.66it/s]\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:04<00:00, 169.45it/s, Materializing param=model.visual.pos_embed.weight]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model, processor = setup_model(\"nvidia/Cosmos-Reason2-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b800f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## B.1 Object Detection in All Videos\n",
    "\n",
    "Using NVIDIA Cosmos Reason 2B to detect and identify objects in all demonstration videos. This model leverages vision-language understanding to provide detailed object detection through natural language descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2971e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 videos in demonstrations folder:\n",
      "  - data10_tracked.mp4\n",
      "  - data1_tracked.mp4\n",
      "  - data2_tracked.mp4\n",
      "  - data3_tracked.mp4\n",
      "  - data4_tracked.mp4\n",
      "  - data5_tracked.mp4\n",
      "  - data6_tracked.mp4\n",
      "  - data7_tracked.mp4\n",
      "  - data8_tracked.mp4\n",
      "  - data9_tracked.mp4\n",
      "  - demonstration1_tracked.mp4\n"
     ]
    }
   ],
   "source": [
    "# Get all video files from the demonstrations folder\n",
    "\n",
    "\n",
    "demonstrations_path = Path(\"demonstrations/objects_tracked\")\n",
    "video_extensions = [\"*.mp4\", \"*.mov\", \"*.avi\", \"*.mkv\"]\n",
    "video_files = []\n",
    "\n",
    "for ext in video_extensions:\n",
    "    video_files.extend(demonstrations_path.glob(ext))\n",
    "\n",
    "video_files = sorted(video_files)\n",
    "print(f\"Found {len(video_files)} videos in demonstrations folder:\")\n",
    "for v in video_files:\n",
    "    print(f\"  - {v.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed871636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing video 1/11: data10_tracked.mp4\n",
      "================================================================================\n",
      " Video: data10_tracked.mp4 | Frames: 555 | FPS: 29.99 | Duration: 18.51 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data10_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data10_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"dice_inventory\": {\n",
      "    \"total_number_of_green_dice\": 1,\n",
      "    \"total_number_of_red_dice\": 1,\n",
      "    \"total_number_of_blue_dice\": 1,\n",
      "    \"total_number_of_all_dice\": 3\n",
      "  },\n",
      "  \"initial_arrangement\": \"The dice are initially placed separately on the table: Green Dice on the left, Red Dice in the center, and Blue Dice on the right. They are distinct and not stacked at the beginning.\",\n",
      "  \"objects_in_scene\": {\n",
      "    \"table_description\": \"A wooden table with a rustic finish, serving as the surface for the ...\n",
      "\n",
      "================================================================================\n",
      "Processing video 2/11: data1_tracked.mp4\n",
      "================================================================================\n",
      " Video: data1_tracked.mp4 | Frames: 854 | FPS: 30.05 | Duration: 28.42 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data1_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data1_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_number_of_green_dice\": 2,\n",
      "  \"total_number_of_red_dice\": 3,\n",
      "  \"total_number_of_blue_dice\": 1,\n",
      "  \"total_number_of_all_dice\": 6\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 3/11: data2_tracked.mp4\n",
      "================================================================================\n",
      " Video: data2_tracked.mp4 | Frames: 620 | FPS: 28.80 | Duration: 21.53 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data2_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data2_tracked.mp4:\n",
      "----------------------------------------\n",
      "Green Dice: 2  \n",
      "Red Dice: 2  \n",
      "Blue Dice: 1  \n",
      "Total Dice: 5\n",
      "\n",
      "================================================================================\n",
      "Processing video 4/11: data3_tracked.mp4\n",
      "================================================================================\n",
      " Video: data3_tracked.mp4 | Frames: 783 | FPS: 29.95 | Duration: 26.15 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data3_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data3_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 5\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 5/11: data4_tracked.mp4\n",
      "================================================================================\n",
      " Video: data4_tracked.mp4 | Frames: 821 | FPS: 29.98 | Duration: 27.39 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data4_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data4_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 3,\n",
      "  \"total_blue_dice\": 2,\n",
      "  \"total_all_dice\": 7\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 6/11: data5_tracked.mp4\n",
      "================================================================================\n",
      " Video: data5_tracked.mp4 | Frames: 592 | FPS: 29.91 | Duration: 19.79 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data5_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data5_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 2,\n",
      "  \"total_all_dice\": 6\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 7/11: data6_tracked.mp4\n",
      "================================================================================\n",
      " Video: data6_tracked.mp4 | Frames: 1229 | FPS: 30.07 | Duration: 40.87 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data6_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data6_tracked.mp4:\n",
      "----------------------------------------\n",
      "<think>\n",
      "Okay, let's see. The user provided  a video where someone is interacting with dice on a table. They want me to analyze it by counting the dice based on specific instructions.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points mention that there are three dice initially: two green and one red. Then, as the person moves them around, they end up stacking all four dice (including the blue one shown later). Wait, the initial count was two green and one red, but then a blue di...\n",
      "\n",
      "================================================================================\n",
      "Processing video 8/11: data7_tracked.mp4\n",
      "================================================================================\n",
      " Video: data7_tracked.mp4 | Frames: 617 | FPS: 29.94 | Duration: 20.61 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data7_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data7_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 1,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 4\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 9/11: data8_tracked.mp4\n",
      "================================================================================\n",
      " Video: data8_tracked.mp4 | Frames: 656 | FPS: 29.96 | Duration: 21.90 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data8_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data8_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 4\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 10/11: data9_tracked.mp4\n",
      "================================================================================\n",
      " Video: data9_tracked.mp4 | Frames: 679 | FPS: 29.88 | Duration: 22.72 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/data9_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in data9_tracked.mp4:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"total_number_of_green_dice\": 1,\n",
      "  \"total_number_of_red_dice\": 1,\n",
      "  \"total_number_of_blue_dice\": 2,\n",
      "  \"total_number_of_all_dice\": 4\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Processing video 11/11: demonstration1_tracked.mp4\n",
      "================================================================================\n",
      " Video: demonstration1_tracked.mp4 | Frames: 584 | FPS: 15.00 | Duration: 38.93 secs\n",
      "\n",
      "Analyzing video: demonstrations/objects_tracked/demonstration1_tracked.mp4\n",
      "Question: You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Count EACH die individually by color (red, green, blue)\n",
      "2. Watch the ENTIRE video to track ALL dice present\n",
      "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
      "4. State the EXACT NUMBER of each color\n",
      "\n",
      "Analyze this video frame-by-frame and provide:\n",
      "\n",
      "**DICE INVENTORY (Count carefully!):**\n",
      "- Total number of GREEN dice: [count each green die you see]\n",
      "- Total number of RED dice: [count each red die you see]  \n",
      "- Total number of BLUE dice: [count each blue die you see]\n",
      "- Total number of ALL dice: [sum]\n",
      "\n",
      "**INITIAL ARRANGEMENT:**\n",
      "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
      "\n",
      "**OBJECTS IN SCENE:**\n",
      "- Table description\n",
      "- Human hands/body parts visible\n",
      "- Any other objects\n",
      "\n",
      "Be precise with your counts. If you see a die being moved, don't count it twice.\n",
      "Number of frames: 500\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " Objects in demonstration1_tracked.mp4:\n",
      "----------------------------------------\n",
      "- Total number of GREEN dice: 2\n",
      "- Total number of RED dice: 2\n",
      "- Total number of BLUE dice: 1\n",
      "- Total number of ALL dice: 5\n"
     ]
    }
   ],
   "source": [
    "# Object Detection Prompt\n",
    "OBJECT_DETECTION_PROMPT = \"\"\"You are analyzing a video with colored dice on a table. Pay EXTREMELY CLOSE ATTENTION to counting.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Count EACH die individually by color (red, green, blue)\n",
    "2. Watch the ENTIRE video to track ALL dice present\n",
    "3. A die may be moved, stacked, or rearranged - count it only ONCE\n",
    "4. State the EXACT NUMBER of each color\n",
    "\n",
    "Analyze this video frame-by-frame and provide:\n",
    "\n",
    "**DICE INVENTORY (Count carefully!):**\n",
    "- Total number of GREEN dice: [count each green die you see]\n",
    "- Total number of RED dice: [count each red die you see]  \n",
    "- Total number of BLUE dice: [count each blue die you see]\n",
    "- Total number of ALL dice: [sum]\n",
    "\n",
    "**INITIAL ARRANGEMENT:**\n",
    "Describe the starting positions of ALL dice from left to right or their spatial arrangement.\n",
    "\n",
    "**OBJECTS IN SCENE:**\n",
    "- Table description\n",
    "- Human hands/body parts visible\n",
    "- Any other objects\n",
    "\n",
    "Be precise with your counts. If you see a die being moved, don't count it twice.\"\"\"\n",
    "\n",
    "# Run object detection on all videos\n",
    "object_detection_results = {}\n",
    "\n",
    "for i, video_path in enumerate(video_files):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing video {i+1}/{len(video_files)}: {video_path.name}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    frame_count = get_video_frame_count(video_path)\n",
    "    \n",
    "    result = analyze_video(\n",
    "        video_path=video_path,\n",
    "        question=OBJECT_DETECTION_PROMPT,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        nframes=frame_count,\n",
    "        enable_reasoning=True\n",
    "    )\n",
    "    \n",
    "    object_detection_results[video_path.name] = {\n",
    "        \"reasoning\": result[\"reasoning\"],\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"full_output\": result[\"full_output\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Objects in {video_path.name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"answer\"][:500] + \"...\" if len(result[\"answer\"]) > 500 else result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54a8a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Video: data10_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video involving colored dice on a table and wants me to analyze it according to specific instructions. My task is to count the number of each colored die (green, red, blue) present in the video while paying close attention to details.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points shown are that there are three dice initially: Green Dice, Red Dice, and Blue Dice. Each has distinct colors and labels. The person interacts with them by moving them around but doesn't add or remove any dice during the process. \n",
      "\n",
      "The actions shown involve picking up the Red Dice, placing it back, then stacking it on the Blue Dice, followed by adding the Green Dice on top of both. Throughout these interactions, the video shows that no addition...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"dice_inventory\": {\n",
      "    \"total_number_of_green_dice\": 1,\n",
      "    \"total_number_of_red_dice\": 1,\n",
      "    \"total_number_of_blue_dice\": 1,\n",
      "    \"total_number_of_all_dice\": 3\n",
      "  },\n",
      "  \"initial_arrangement\": \"The dice are initially placed separately on the table: Green Dice on the left, Red Dice in the center, and Blue Dice on the right. They are distinct and not stacked at the beginning.\",\n",
      "  \"objects_in_scene\": {\n",
      "    \"table_description\": \"A wooden table with a rustic finish, serving as the surface for the dice activity.\",\n",
      "    \"human_body_parts_visible\": \"Hands of a person interacting with the dice, wearing a patterned shirt sleeve.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data1_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video involving colored dice on a table and wants me to analyze it based on specific instructions.\n",
      "\n",
      "First, I need to parse the video carefully. The key points mention that there are three red dice, two green dice, and one blue die initially. Then, during the interaction, the person moves the blue die and stacks the red ones. Later, they stack the green dice on top of the red stack. \n",
      "\n",
      "The critical part here is ensuring that each die is counted only once, even if they're moved or stacked. The initial count is three red, two green, one blue. When stacking happens, the numbers don't change; they just change positions. So the total should remain the same: three red, two green, one blue, totaling six dice.\n",
      "\n",
      "I should check if any dice are added or...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_number_of_green_dice\": 2,\n",
      "  \"total_number_of_red_dice\": 3,\n",
      "  \"total_number_of_blue_dice\": 1,\n",
      "  \"total_number_of_all_dice\": 6\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data2_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video involving dice manipulation on a wooden table. My task is to count the number of each colored die (green, red, blue) based on the actions shown.\n",
      "\n",
      "First, I need to track the initial setup and subsequent movements. Initially, there are two green dice, one red die, and one blue die. As the person interacts with them, they move and stack the dice. For example, they place a green die near the center, then stack a red die on top of it, followed by a blue die. Then they move another red die closer and stack it on top of the blue one. Finally, they take a green die from the left side and place it on top of the stack.\n",
      "\n",
      "Throughout these actions, even though dice are moved and stacked, each die is only counted once because they're not duplicated...\n",
      "\n",
      "Objects Detected:\n",
      "Green Dice: 2  \n",
      "Red Dice: 2  \n",
      "Blue Dice: 1  \n",
      "Total Dice: 5\n",
      "\n",
      "============================================================\n",
      "Video: data3_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video showing a sequence of events involving colored dice on a table. The task was to count the number of each colored die and provide an inventory based on their movements.\n",
      "\n",
      "First, I need to parse the given information carefully. The video shows that there are initially five dice: two green, two red, and one blue. Each die is moved one by one from their original positions to form a vertical stack. The key points here are the initial counts and the process of stacking them without duplication.\n",
      "\n",
      "Looking at the steps shown, each die is picked up sequentially and placed on top of another. Since each action involves moving a single die, the total count should remain constant throughout the video. The critical part is ensuring that no die is cou...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 5\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data4_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's see. The user provided  a video involving colored dice on a wooden table. The task is to count the number of each colored die (green, red, blue) and provide the total.\n",
      "\n",
      "First, I need to parse through the video carefully. The initial setup mentions three dice: one blue, one green, and one red. They're arranged in a row on the left side of the table. Then there's mention of additional dice being added by a person's hand. Specifically, two more red dice are placed next to the existing ones, followed by a blue die and another green die. \n",
      "\n",
      "So starting with the initial count: 1 blue, 1 green, 1 red. Then adding two red dice makes it three red total. Adding a blue die brings the blue count to two. Adding another green die makes it two green total. Let me check again to ensure no dice ...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 3,\n",
      "  \"total_blue_dice\": 2,\n",
      "  \"total_all_dice\": 7\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data5_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's see. The user provided  a video involving colored dice on a wooden table. The task was to count the number of each colored die (green, red, blue) and provide the total.\n",
      "\n",
      "First, I need to parse the video carefully. Initially, there are three dice: one green, one red, and one blue. Then, during the interaction, additional dice appear. Specifically, another green die is added when the person moves it from the left side to the center. Later, more red and blue dice are stacked on top of each other. \n",
      "\n",
      "I should track each die's movement without double-counting. The key points are:\n",
      "- Starting with 1 green, 1 red, 1 blue.\n",
      "- Adding another green die (now 2 green).\n",
      "- Stacking red and blue dice (so those remain as individual counts but are part of a stack).\n",
      "\n",
      "The final counts should be:\n",
      "- G...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 2,\n",
      "  \"total_all_dice\": 6\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data6_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "\n",
      "Objects Detected:\n",
      "<think>\n",
      "Okay, let's see. The user provided  a video where someone is interacting with dice on a table. They want me to analyze it by counting the dice based on specific instructions.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points mention that there are three dice initially: two green and one red. Then, as the person moves them around, they end up stacking all four dice (including the blue one shown later). Wait, the initial count was two green and one red, but then a blue die appears when stacking. So maybe the blue die was already present but not counted in the initial description? Or perhaps it was introduced during the interaction.\n",
      "\n",
      "Looking at the steps: the person picks up the top green die, places it aside, then moves the red die onto another green die, creating a stack of two. Then they add the blue die on top, making three. But the initial setup only had two green and one red. Where did the blue come from? Maybe it was part of the initial setup but not explicitly shown. The user's instructions say to count each die by color, so even if it wasn't shown initially, if it's present in the scene, it should be counted.\n",
      "\n",
      "The final stack has four dice: green, red, blue, green. Wait, no-the video shows after placing the blue die on top of the red and green stack, forming a tower of three. Then the last action is placing the blue die on top of the red die, which was already on the green die. Hmm, maybe the blue die was part of the initial setup. Let me check again.\n",
      "\n",
      "In the initial state: two green, one red. Then during interaction, the person stacks them as green, red, blue, green? Wait, the video shows \"placing the blue die on top of the red and green stack, forming a tower of three.\" So starting with two green and one red, stacking red on green makes two, then adding blue makes three. But where is the fourth die? Maybe the initial count was three dice (two green, one red), and the blue die was part of the stack. Wait, the user's question is about counting the dice present in the video. The initial description mentions two green and one red. Then, during the interaction, the blue die is used. So perhaps the blue die was already on the table but not shown in the initial setup. That would make four dice total: two green, one red, one blue. But the user's instructions say to count each die by color. So even if it wasn't shown initially, if it's present, it should be counted.\n",
      "\n",
      "Wait, the video shows \"the blue die\" when stacking. So maybe the blue die was part of the initial setup. Let me re-examine the initial description. It says \"two green dice and one red die.\" So three dice. Then, during the interaction, the person uses a blue die. So perhaps the blue die was part of the initial setup but not shown. Or maybe it was introduced later. This is a bit confusing. The user's instructions say to count all dice present in the video. So if the blue die is present, even if it wasn't shown in the initial setup, it should be counted. However, according to the given environment, the initial setup has two green and one red. Then, during the interaction, the person stacks them as green, red, blue, green. Wait, that would require four dice. So maybe the initial count was three, and the blue die was part of the stack. But the user's question is about the exact number of each color in the video. So if the blue die is present, even if it was part of the stack, it should be counted. Therefore, the total would be two green, one red, one blue. But the user's answer format requires counts for green, red, blue, and total. So the answer would be green: 2, red:1, blue:1, total:4. But wait, in the interaction, after stacking, the final arrangement is a tower of three: green, red, blue. But where is the fourth die? Maybe the initial count was three, and the blue die was part of the stack. Alternatively, maybe the initial setup had four dice, but the video shows two green and one red. Hmm. This is a bit conflicting. Alternatively, perhaps the blue die was part of the initial setup but not shown. The user's instructions say to count each die by color. So if the blue die is present, even if it wasn't shown in the initial setup, it should be counted. Therefore, the correct counts would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3, red:3, blue:1. Wait, that's different. Let me check again. The video shows that the person stacks the dice as green, red, blue, green. So four dice. Therefore, the initial setup must have had four dice: two green, one red, and one blue. But the initial description says two green and one red. So perhaps the blue die was part of the initial setup but not shown. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. That suggests that maybe the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die. But that doesn't make sense because the video shows the person uses a blue die. Alternatively, maybe the blue die was part of the initial setup. The user's instructions say to count all dice present. So if the blue die is present, even if it wasn't shown in the initial setup, it should be counted. Therefore, the answer should be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Hmm. Alternatively, maybe the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die. But that would make four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user made a mistake. Alternatively, perhaps the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user's example is incorrect, but I need to follow the given information. Wait, the video shows \"the blue die\" when stacking. So the blue die must be present. Therefore, the initial setup must have included the blue die. But the initial description says two green and one red. So perhaps the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, maybe the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is confusing. Maybe the user made a mistake in the video. Alternatively, perhaps the blue die was part of the initial setup but not shown. In that case, the correct counts would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Hmm. Alternatively, maybe the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This discrepancy suggests that perhaps the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user made a mistake in their example. Alternatively, perhaps the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This is confusing. Alternatively, maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue die. But the initial description says two green and one red. Therefore, the initial setup had three dice (two green, one red), and the blue die was part of the stack. But that doesn't add up. Alternatively, perhaps the initial setup had four dice: two green, one red, and one blue. But the initial description says two green and one red. This is conflicting. Given this confusion, perhaps the safest approach is to count the dice as shown in the interaction. The person starts with two green and one red. Then, during the interaction, they use a blue die. So the total would be two green, one red, and one blue, totaling four. Therefore, the answer would be green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. Maybe the user intended that the initial setup had three dice (two green, one red), and during the interaction, the person adds a blue die, making four. But the video shows the person uses a blue die. So perhaps the blue die was part of the initial setup. Therefore, the correct counts are green:2, red:1, blue:1, total:4. But the user's example answer shows green:3. This suggests that perhaps the user made a mistake in their example. However, I need to follow the given information. The video shows that the person uses a blue die. Therefore, the blue die must be present. So the initial setup must have included the blue\n",
      "\n",
      "============================================================\n",
      "Video: data7_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video involving colored dice on a table and asked for an analysis focusing on counting each die by color.\n",
      "\n",
      "First, I need to parse the information given about the dice. The video shows three main dice: Green Dice, Blue Dice, and Red Dice. There's also mention of an Unknown Dice that appears briefly but is noted as not being counted because its identity isn't confirmed.\n",
      "\n",
      "The key points here are the initial arrangement and subsequent movements. Initially, there are two Green Dice, one Blue Dice, and one Red Dice. The person moves the Red Dice to the right, then places a Green Dice on top of it. Then they move the Blue Dice onto the stack, followed by another Green Dice. Finally, the Red Dice is placed on top of the stack.\n",
      "\n",
      "Throughout these act...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 1,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 4\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data8_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's break this down. The user provided  a video involving colored dice on a table and wants me to analyze it by counting each die by color.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points shown are that there are four dice initially: two red, one green, and one blue. These are arranged on a wooden table. Then, throughout the video, the person moves them around, stacking them into a vertical column. \n",
      "\n",
      "The critical part here is ensuring that even though the dice are moved and stacked, they're only counted once per color. The initial setup has two red dice, so regardless of how they're rearranged later, their count remains two. Similarly, there's one green and one blue die each. Even when they're stacked vertically, those counts don't change because stacking doesn'...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_green_dice\": 2,\n",
      "  \"total_red_dice\": 2,\n",
      "  \"total_blue_dice\": 1,\n",
      "  \"total_all_dice\": 4\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: data9_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's see. The user provided  a video where someone is interacting with colored dice on a wooden table. They want me to analyze the video and provide specific counts for each color of dice.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points shown are that there are four dice initially: one red, one green, and two blue ones. Then, as the person moves them around, they stack them into a tower. The actions include moving each die one by one and placing them on top of each other until all four are stacked.\n",
      "\n",
      "The critical part here is ensuring that each die is counted only once, even if they're moved or stacked. Since the video shows four dice total (red, green, two blue), stacking them shouldn't change the count. Each die remains a single entity regardless of its position...\n",
      "\n",
      "Objects Detected:\n",
      "{\n",
      "  \"total_number_of_green_dice\": 1,\n",
      "  \"total_number_of_red_dice\": 1,\n",
      "  \"total_number_of_blue_dice\": 2,\n",
      "  \"total_number_of_all_dice\": 4\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Video: demonstration1_tracked.mp4\n",
      "============================================================\n",
      "\n",
      "Reasoning:\n",
      "Okay, let's see. The user provided  a video showing a person interacting with colored dice on a wooden table. The task is to analyze the video and provide a precise count of each colored die.\n",
      "\n",
      "First, I need to parse through the video carefully. The key points shown are that there are four dice initially: two red, one green, and one blue. The person moves them around but doesn't add or remove any dice during the process. The actions involve picking up and stacking the dice, specifically placing the blue on red, then another red on top, followed by a green, and finally another green on top. \n",
      "\n",
      "I should check if any dice are added or removed. The video shows that the person uses their hands to move the dice but doesn't mention any new dice entering the scene or existing ones being taken away. ...\n",
      "\n",
      "Objects Detected:\n",
      "- Total number of GREEN dice: 2\n",
      "- Total number of RED dice: 2\n",
      "- Total number of BLUE dice: 1\n",
      "- Total number of ALL dice: 5\n"
     ]
    }
   ],
   "source": [
    "# Display complete object detection results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for video_name, data in object_detection_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nReasoning:\")\n",
    "    print(data[\"reasoning\"][:800] + \"...\" if len(data[\"reasoning\"]) > 800 else data[\"reasoning\"])\n",
    "    print(\"\\nObjects Detected:\")\n",
    "    print(data[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6559",
   "metadata": {},
   "source": [
    "## B.2 Human Action Recognition\n",
    "\n",
    "Using NVIDIA Cosmos Reason 2B to recognize actions performed in the videos (grasping, moving, releasing, stacking, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing video 1/11: data1.mov\n",
      "================================================================================\n",
      "\n",
      "Analyzing video: demonstrations/data1.mov\n",
      "Question: Analyze this video and identify ALL human actions performed.\n",
      "\n",
      "For each action, provide:\n",
      "1. Action name (e.g., grasping, picking up, moving, placing, releasing, stacking, pushing, pulling)\n",
      "2. The object involved in the action\n",
      "3. Approximate timing (beginning, middle, end of video)\n",
      "4. Hand used (left, right, or both)\n",
      "\n",
      "Focus on fine-grained manipulation actions such as:\n",
      "- Reaching/approaching\n",
      "- Grasping/gripping\n",
      "- Lifting/picking up\n",
      "- Moving/transporting\n",
      "- Placing/positioning\n",
      "- Releasing/letting go\n",
      "- Stacking/arranging\n",
      "- Adjusting/fine-tuning position\n",
      "\n",
      "Provide a chronological list of all actions observed.\n",
      "Number of frames: 60\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " ACTIONS RECOGNIZED in data1.mov:\n",
      "----------------------------------------\n",
      "The individual stacks four dice (red, blue, green, and another green) vertically on the table using their right hand.\n",
      "\n",
      "================================================================================\n",
      "Processing video 2/11: data10.mov\n",
      "================================================================================\n",
      "\n",
      "Analyzing video: demonstrations/data10.mov\n",
      "Question: Analyze this video and identify ALL human actions performed.\n",
      "\n",
      "For each action, provide:\n",
      "1. Action name (e.g., grasping, picking up, moving, placing, releasing, stacking, pushing, pulling)\n",
      "2. The object involved in the action\n",
      "3. Approximate timing (beginning, middle, end of video)\n",
      "4. Hand used (left, right, or both)\n",
      "\n",
      "Focus on fine-grained manipulation actions such as:\n",
      "- Reaching/approaching\n",
      "- Grasping/gripping\n",
      "- Lifting/picking up\n",
      "- Moving/transporting\n",
      "- Placing/positioning\n",
      "- Releasing/letting go\n",
      "- Stacking/arranging\n",
      "- Adjusting/fine-tuning position\n",
      "\n",
      "Provide a chronological list of all actions observed.\n",
      "Number of frames: 60\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " ACTIONS RECOGNIZED in data10.mov:\n",
      "----------------------------------------\n",
      "The person uses their right hand to manipulate the dice, rolling them one by one from left to right across the table. The sequence involves picking up each die individually and placing them in a line. The actions occur sequentially without any overlap between the dice.\n",
      "\n",
      "================================================================================\n",
      "Processing video 3/11: data2.mov\n",
      "================================================================================\n",
      "\n",
      "Analyzing video: demonstrations/data2.mov\n",
      "Question: Analyze this video and identify ALL human actions performed.\n",
      "\n",
      "For each action, provide:\n",
      "1. Action name (e.g., grasping, picking up, moving, placing, releasing, stacking, pushing, pulling)\n",
      "2. The object involved in the action\n",
      "3. Approximate timing (beginning, middle, end of video)\n",
      "4. Hand used (left, right, or both)\n",
      "\n",
      "Focus on fine-grained manipulation actions such as:\n",
      "- Reaching/approaching\n",
      "- Grasping/gripping\n",
      "- Lifting/picking up\n",
      "- Moving/transporting\n",
      "- Placing/positioning\n",
      "- Releasing/letting go\n",
      "- Stacking/arranging\n",
      "- Adjusting/fine-tuning position\n",
      "\n",
      "Provide a chronological list of all actions observed.\n",
      "Number of frames: 60\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n",
      "\n",
      " ACTIONS RECOGNIZED in data2.mov:\n",
      "----------------------------------------\n",
      "The person uses their right hand to pick up and place the green dice in a straight line on the table.\n",
      "\n",
      "================================================================================\n",
      "Processing video 4/11: data3.mov\n",
      "================================================================================\n",
      "\n",
      "Analyzing video: demonstrations/data3.mov\n",
      "Question: Analyze this video and identify ALL human actions performed.\n",
      "\n",
      "For each action, provide:\n",
      "1. Action name (e.g., grasping, picking up, moving, placing, releasing, stacking, pushing, pulling)\n",
      "2. The object involved in the action\n",
      "3. Approximate timing (beginning, middle, end of video)\n",
      "4. Hand used (left, right, or both)\n",
      "\n",
      "Focus on fine-grained manipulation actions such as:\n",
      "- Reaching/approaching\n",
      "- Grasping/gripping\n",
      "- Lifting/picking up\n",
      "- Moving/transporting\n",
      "- Placing/positioning\n",
      "- Releasing/letting go\n",
      "- Stacking/arranging\n",
      "- Adjusting/fine-tuning position\n",
      "\n",
      "Provide a chronological list of all actions observed.\n",
      "Number of frames: 60\n",
      "Reasoning enabled: True\n",
      "\n",
      "Generating response...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing video \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m result = \u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mACTION_RECOGNITION_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnframes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_reasoning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m action_recognition_results[video_path.name] = {\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     43\u001b[39m }\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m ACTIONS RECOGNIZED in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36manalyze_video\u001b[39m\u001b[34m(video_path, question, model, processor, nframes, enable_reasoning)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Recommended to avoid truncation\u001b[39;49;00m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Trim the input tokens from the generated output\u001b[39;00m\n\u001b[32m     87\u001b[39m generated_ids_trimmed = [\n\u001b[32m     88\u001b[39m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids):] \n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)\n\u001b[32m     90\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/generation/utils.py:2638\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2637\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/generation/utils.py:2843\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2842\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optimize_model_for_decode():\n\u001b[32m-> \u001b[39m\u001b[32m2843\u001b[39m         outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m prefill_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2845\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2846\u001b[39m     outputs,\n\u001b[32m   2847\u001b[39m     model_kwargs,\n\u001b[32m   2848\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2849\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/utils/generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1438\u001b[39m, in \u001b[36mQwen3VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1389\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1390\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m | Qwen3VLCausalLMOutputWithPast:\n\u001b[32m   1391\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1392\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1393\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1435\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   1436\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1452\u001b[39m     hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1454\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1274\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1271\u001b[39m         position_ids = position_ids.add(delta)\n\u001b[32m   1272\u001b[39m         position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand(\u001b[32m3\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisual_pos_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisual_pos_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeepstack_visual_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeepstack_visual_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Qwen3VLModelOutputWithPast(\n\u001b[32m   1287\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m   1288\u001b[39m     past_key_values=outputs.past_key_values,\n\u001b[32m   1289\u001b[39m     rope_deltas=\u001b[38;5;28mself\u001b[39m.rope_deltas,\n\u001b[32m   1290\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:911\u001b[39m, in \u001b[36mQwen3VLTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, visual_pos_masks, deepstack_visual_embeds, **kwargs)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m     hidden_states = layer_outputs\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# add visual features to the hidden states of first several layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:544\u001b[39m, in \u001b[36mQwen3VLTextDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    543\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:499\u001b[39m, in \u001b[36mQwen3VLTextAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m attn_output, attn_weights = attention_interface(\n\u001b[32m    488\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    489\u001b[39m     query_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    495\u001b[39m     **kwargs,\n\u001b[32m    496\u001b[39m )\n\u001b[32m    498\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SCU/ECEN524/Cosmos-8b/cosmos_env/lib/python3.13/site-packages/accelerate/utils/modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Action Recognition Prompt\n",
    "ACTION_RECOGNITION_PROMPT = \"\"\"Analyze this video and identify ALL human actions performed required to complete the pattern of dice in this video. The dice will be lined up in a specific pattern.\n",
    "\n",
    "For each action, provide:\n",
    "1. Action name (e.g., grasping, picking up, moving, placing, releasing, stacking, pushing, pulling)\n",
    "2. The object involved in the action\n",
    "3. The state of the cube's pattern in a line, after each action is preformed\n",
    "\n",
    "Focus on fine-grained manipulation actions such as:\n",
    "- Reaching/approaching\n",
    "- Grasping/gripping\n",
    "- Lifting/picking up\n",
    "- Moving/transporting\n",
    "- Placing/positioning\n",
    "- Releasing/letting go\n",
    "- Stacking/arranging\n",
    "- Adjusting/fine-tuning position\n",
    "\n",
    "Provide a chronological list of all actions observed.\"\"\"\n",
    "\n",
    "# Run action recognition on all videos\n",
    "action_recognition_results = {}\n",
    "\n",
    "for i, video_path in enumerate(video_files):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing video {i+1}/{len(video_files)}: {video_path.name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = analyze_video(\n",
    "        video_path=video_path,\n",
    "        question=ACTION_RECOGNITION_PROMPT,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        nframes=60,\n",
    "        enable_reasoning=True\n",
    "    )\n",
    "    \n",
    "    action_recognition_results[video_path.name] = {\n",
    "        \"reasoning\": result[\"reasoning\"],\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"full_output\": result[\"full_output\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Actions in {video_path.name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"answer\"][:500] + \"...\" if len(result[\"answer\"]) > 500 else result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete action recognition results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for video_name, data in action_recognition_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nReasoning:\")\n",
    "    print(data[\"reasoning\"][:800] + \"...\" if len(data[\"reasoning\"]) > 800 else data[\"reasoning\"])\n",
    "    print(\"\\nActions:\")\n",
    "    print(data[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29b508",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Automatic Generation of Sequence of Actions\n",
    "\n",
    "## C.1 Action Sequence Generation\n",
    "\n",
    "Using NVIDIA Cosmos Reason 2B to generate structured sequences of actions from all videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7631be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Sequence Generation Prompt\n",
    "ACTION_SEQUENCE_PROMPT = \"\"\"Analyze this video and generate a STRUCTURED SEQUENCE of actions.\n",
    "\n",
    "Output the sequence in the following JSON-like format for each action:\n",
    "{\n",
    "  \"step\": <step_number>,\n",
    "  \"action\": \"<action_verb>\",\n",
    "  \"object\": \"<object_being_manipulated>\",\n",
    "  \"start_state\": \"<state_before_action>\",\n",
    "  \"end_state\": \"<state_after_action>\",\n",
    "  \"preconditions\": [\"<required_conditions>\"],\n",
    "  \"effects\": [\"<resulting_changes>\"]\n",
    "}\n",
    "\n",
    "Use ONLY these standardized action verbs:\n",
    "- REACH: Moving hand toward an object\n",
    "- GRASP: Closing fingers around an object\n",
    "- LIFT: Raising an object from a surface\n",
    "- MOVE: Transporting an object through space\n",
    "- PLACE: Positioning an object at a location\n",
    "- RELEASE: Opening fingers to let go of object\n",
    "- ADJUST: Fine-tuning object position\n",
    "- STACK: Placing object on top of another\n",
    "\n",
    "Generate the complete action sequence from start to finish.\"\"\"\n",
    "\n",
    "# Run action sequence generation on all videos\n",
    "action_sequence_results = {}\n",
    "\n",
    "for i, video_path in enumerate(video_files):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing video {i+1}/{len(video_files)}: {video_path.name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = analyze_video(\n",
    "        video_path=video_path,\n",
    "        question=ACTION_SEQUENCE_PROMPT,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        nframes=60,\n",
    "        enable_reasoning=True\n",
    "    )\n",
    "    \n",
    "    action_sequence_results[video_path.name] = {\n",
    "        \"reasoning\": result[\"reasoning\"],\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"full_output\": result[\"full_output\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ ACTION SEQUENCE for {video_path.name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"answer\"][:600] + \"...\" if len(result[\"answer\"]) > 600 else result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete action sequence results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for video_name, data in action_sequence_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nAction Sequence:\")\n",
    "    print(data[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201befd8",
   "metadata": {},
   "source": [
    "## C.2 Markov Decision Process Design\n",
    "\n",
    "Based on the observed action sequences, we design a Markov Decision Process (MDP) for the manipulation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components based on observed actions\n",
    "\n",
    "# States: Represent the configuration of objects and hand state\n",
    "STATES = {\n",
    "    \"S0_IDLE\": \"Hand empty, objects on table (initial state)\",\n",
    "    \"S1_REACHING\": \"Hand moving toward target object\",\n",
    "    \"S2_GRASPING\": \"Hand closing around object\",\n",
    "    \"S3_HOLDING\": \"Object grasped and held\",\n",
    "    \"S4_MOVING\": \"Object being transported\",\n",
    "    \"S5_POSITIONING\": \"Object at target location\",\n",
    "    \"S6_RELEASING\": \"Hand opening to release object\",\n",
    "    \"S7_STACKED\": \"Object placed on stack (goal state)\",\n",
    "    \"S8_COMPLETE\": \"All objects stacked (terminal state)\"\n",
    "}\n",
    "\n",
    "# Actions: Possible actions the agent can take\n",
    "ACTIONS = {\n",
    "    \"A0_WAIT\": \"Do nothing, remain in current state\",\n",
    "    \"A1_REACH\": \"Extend hand toward target object\",\n",
    "    \"A2_GRASP\": \"Close fingers around object\",\n",
    "    \"A3_LIFT\": \"Raise object from surface\",\n",
    "    \"A4_MOVE\": \"Transport object to target location\",\n",
    "    \"A5_LOWER\": \"Move object down toward surface\",\n",
    "    \"A6_RELEASE\": \"Open fingers to let go\",\n",
    "    \"A7_ADJUST\": \"Fine-tune object position\"\n",
    "}\n",
    "\n",
    "# Transition Probabilities (estimated from video observations)\n",
    "# Format: P(next_state | current_state, action)\n",
    "TRANSITION_PROBS = {\n",
    "    (\"S0_IDLE\", \"A1_REACH\"): {\"S1_REACHING\": 0.95, \"S0_IDLE\": 0.05},\n",
    "    (\"S1_REACHING\", \"A2_GRASP\"): {\"S2_GRASPING\": 0.90, \"S1_REACHING\": 0.10},\n",
    "    (\"S2_GRASPING\", \"A3_LIFT\"): {\"S3_HOLDING\": 0.95, \"S2_GRASPING\": 0.05},\n",
    "    (\"S3_HOLDING\", \"A4_MOVE\"): {\"S4_MOVING\": 0.90, \"S3_HOLDING\": 0.10},\n",
    "    (\"S4_MOVING\", \"A5_LOWER\"): {\"S5_POSITIONING\": 0.85, \"S4_MOVING\": 0.15},\n",
    "    (\"S5_POSITIONING\", \"A6_RELEASE\"): {\"S6_RELEASING\": 0.90, \"S5_POSITIONING\": 0.10},\n",
    "    (\"S6_RELEASING\", \"A0_WAIT\"): {\"S7_STACKED\": 0.95, \"S0_IDLE\": 0.05},\n",
    "    (\"S5_POSITIONING\", \"A7_ADJUST\"): {\"S5_POSITIONING\": 0.70, \"S7_STACKED\": 0.30},\n",
    "}\n",
    "\n",
    "# Rewards\n",
    "REWARDS = {\n",
    "    \"S7_STACKED\": 10.0,      # Successfully stacked one object\n",
    "    \"S8_COMPLETE\": 100.0,    # All objects stacked (task complete)\n",
    "    \"S0_IDLE\": -0.1,         # Small penalty for idle\n",
    "    \"FAILED_GRASP\": -5.0,    # Failed grasp attempt\n",
    "    \"DROPPED\": -10.0,        # Dropped object\n",
    "    \"DEFAULT\": -0.5          # Step cost to encourage efficiency\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MARKOV DECISION PROCESS DEFINITION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nStates:\")\n",
    "for state, desc in STATES.items():\n",
    "    print(f\"  {state}: {desc}\")\n",
    "\n",
    "print(\"\\nActions:\")\n",
    "for action, desc in ACTIONS.items():\n",
    "    print(f\"  {action}: {desc}\")\n",
    "\n",
    "print(\"\\nTransition Probabilities (sample):\")\n",
    "for (state, action), probs in list(TRANSITION_PROBS.items())[:5]:\n",
    "    print(f\"  P(Â·|{state}, {action}):\")\n",
    "    for next_state, prob in probs.items():\n",
    "        print(f\"    â†’ {next_state}: {prob:.2f}\")\n",
    "\n",
    "print(\"\\nRewards:\")\n",
    "for state, reward in REWARDS.items():\n",
    "    print(f\"  {state}: {reward:+.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1681c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MDP Diagram using graphviz-style text representation\n",
    "# (Can be visualized with graphviz or mermaid)\n",
    "\n",
    "# mdp_diagram = \"\"\"\n",
    "# MDP State Transition Diagram (Mermaid format - paste into mermaid.live):\n",
    "\n",
    "# ```mermaid\n",
    "# stateDiagram-v2\n",
    "#     [*] --> S0_IDLE\n",
    "    \n",
    "#     S0_IDLE --> S1_REACHING : A1_REACH (0.95)\n",
    "#     S0_IDLE --> S0_IDLE : A0_WAIT (1.0)\n",
    "    \n",
    "#     S1_REACHING --> S2_GRASPING : A2_GRASP (0.90)\n",
    "#     S1_REACHING --> S1_REACHING : fail (0.10)\n",
    "    \n",
    "#     S2_GRASPING --> S3_HOLDING : A3_LIFT (0.95)\n",
    "#     S2_GRASPING --> S0_IDLE : fail (0.05)\n",
    "    \n",
    "#     S3_HOLDING --> S4_MOVING : A4_MOVE (0.90)\n",
    "#     S3_HOLDING --> S3_HOLDING : hold (0.10)\n",
    "    \n",
    "#     S4_MOVING --> S5_POSITIONING : A5_LOWER (0.85)\n",
    "#     S4_MOVING --> S4_MOVING : adjust (0.15)\n",
    "    \n",
    "#     S5_POSITIONING --> S6_RELEASING : A6_RELEASE (0.90)\n",
    "#     S5_POSITIONING --> S5_POSITIONING : A7_ADJUST (0.70)\n",
    "#     S5_POSITIONING --> S7_STACKED : A7_ADJUST (0.30)\n",
    "    \n",
    "#     S6_RELEASING --> S7_STACKED : success (0.95)\n",
    "#     S6_RELEASING --> S0_IDLE : dropped (0.05)\n",
    "    \n",
    "#     S7_STACKED --> S0_IDLE : next_object\n",
    "#     S7_STACKED --> S8_COMPLETE : all_done\n",
    "    \n",
    "#     S8_COMPLETE --> [*]\n",
    "    \n",
    "#     note right of S7_STACKED : Reward: +10\n",
    "#     note right of S8_COMPLETE : Reward: +100\n",
    "# ```\n",
    "\n",
    "# Rewards:\n",
    "# - S7_STACKED (object placed): +10\n",
    "# - S8_COMPLETE (all done): +100\n",
    "# - Failed transitions: -5 to -10\n",
    "# - Each step: -0.5 (encourages efficiency)\n",
    "# \"\"\"\n",
    "\n",
    "# print(mdp_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of the MDP using matplotlib\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# State positions (arranged in a flow)\n",
    "state_positions = {\n",
    "    \"S0_IDLE\": (1, 5),\n",
    "    \"S1_REACHING\": (3, 5),\n",
    "    \"S2_GRASPING\": (5, 5),\n",
    "    \"S3_HOLDING\": (7, 5),\n",
    "    \"S4_MOVING\": (9, 5),\n",
    "    \"S5_POSITIONING\": (11, 5),\n",
    "    \"S6_RELEASING\": (13, 5),\n",
    "    \"S7_STACKED\": (11, 2),\n",
    "    \"S8_COMPLETE\": (13, 2),\n",
    "}\n",
    "\n",
    "# Draw states\n",
    "for state, (x, y) in state_positions.items():\n",
    "    if state == \"S8_COMPLETE\":\n",
    "        color = 'lightgreen'\n",
    "    elif state == \"S7_STACKED\":\n",
    "        color = 'lightblue'\n",
    "    elif state == \"S0_IDLE\":\n",
    "        color = 'lightyellow'\n",
    "    else:\n",
    "        color = 'lightgray'\n",
    "    \n",
    "    circle = plt.Circle((x, y), 0.5, color=color, ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    # State label\n",
    "    short_name = state.replace(\"S\", \"\").replace(\"_\", \"\\n\")\n",
    "    ax.text(x, y, short_name, ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Draw transitions (arrows)\n",
    "transitions = [\n",
    "    (\"S0_IDLE\", \"S1_REACHING\", \"REACH\"),\n",
    "    (\"S1_REACHING\", \"S2_GRASPING\", \"GRASP\"),\n",
    "    (\"S2_GRASPING\", \"S3_HOLDING\", \"LIFT\"),\n",
    "    (\"S3_HOLDING\", \"S4_MOVING\", \"MOVE\"),\n",
    "    (\"S4_MOVING\", \"S5_POSITIONING\", \"LOWER\"),\n",
    "    (\"S5_POSITIONING\", \"S6_RELEASING\", \"RELEASE\"),\n",
    "    (\"S6_RELEASING\", \"S7_STACKED\", \"0.95\"),\n",
    "    (\"S7_STACKED\", \"S0_IDLE\", \"next\"),\n",
    "    (\"S7_STACKED\", \"S8_COMPLETE\", \"done\"),\n",
    "]\n",
    "\n",
    "for start, end, label in transitions:\n",
    "    x1, y1 = state_positions[start]\n",
    "    x2, y2 = state_positions[end]\n",
    "    \n",
    "    # Calculate direction\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    # Offset to start/end at circle edge\n",
    "    offset = 0.55\n",
    "    x1_adj = x1 + offset * dx / dist\n",
    "    y1_adj = y1 + offset * dy / dist\n",
    "    x2_adj = x2 - offset * dx / dist\n",
    "    y2_adj = y2 - offset * dy / dist\n",
    "    \n",
    "    ax.annotate(\"\", xy=(x2_adj, y2_adj), xytext=(x1_adj, y1_adj),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color='darkblue', lw=1.5))\n",
    "    \n",
    "    # Label\n",
    "    mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2 + 0.3\n",
    "    ax.text(mid_x, mid_y, label, ha='center', va='bottom', fontsize=7, color='darkred')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='lightyellow', edgecolor='black', label='Initial State'),\n",
    "    mpatches.Patch(facecolor='lightgray', edgecolor='black', label='Intermediate State'),\n",
    "    mpatches.Patch(facecolor='lightblue', edgecolor='black', label='Reward State (+10)'),\n",
    "    mpatches.Patch(facecolor='lightgreen', edgecolor='black', label='Terminal State (+100)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower left', fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 15)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Markov Decision Process for Object Manipulation Task', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mdp_diagram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved as 'mdp_diagram.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63941d21",
   "metadata": {},
   "source": [
    "## Limitations Discussion\n",
    "\n",
    "### Limitations of the Action Sequence Generation Approach:\n",
    "\n",
    "1. **Temporal Resolution**: The model samples at a fixed FPS (4 frames/second), which may miss rapid or subtle actions.\n",
    "\n",
    "2. **Vocabulary Constraints**: Actions are limited to predefined verbs; novel or complex manipulations may not be accurately captured.\n",
    "\n",
    "3. **Context Dependency**: The model relies on visual cues only; it cannot infer intent, force, or tactile feedback.\n",
    "\n",
    "4. **Generalization**: The approach is trained on specific manipulation scenarios; extending to other domains (e.g., cooking, assembly) may require prompt engineering or fine-tuning.\n",
    "\n",
    "5. **Occlusion Handling**: When hands or objects are occluded, the model may make incorrect inferences.\n",
    "\n",
    "6. **Multi-object Tracking**: With many similar objects, the model may confuse object identities across frames.\n",
    "\n",
    "7. **Real-time Performance**: The current approach is not suitable for real-time applications due to inference latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON files for reference\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save object detection results\n",
    "with open(f\"object_detection_results_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(object_detection_results, f, indent=2)\n",
    "print(f\"Object detection results saved to object_detection_results_{timestamp}.json\")\n",
    "\n",
    "# Save action recognition results\n",
    "with open(f\"action_recognition_results_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(action_recognition_results, f, indent=2)\n",
    "print(f\"Action recognition results saved to action_recognition_results_{timestamp}.json\")\n",
    "\n",
    "# Save action sequence results\n",
    "with open(f\"action_sequence_results_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(action_sequence_results, f, indent=2)\n",
    "print(f\"Action sequence results saved to action_sequence_results_{timestamp}.json\")\n",
    "\n",
    "# Save MDP definition\n",
    "mdp_definition = {\n",
    "    \"states\": STATES,\n",
    "    \"actions\": ACTIONS,\n",
    "    \"transition_probabilities\": {f\"{s}_{a}\": p for (s, a), p in TRANSITION_PROBS.items()},\n",
    "    \"rewards\": REWARDS\n",
    "}\n",
    "with open(f\"mdp_definition_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(mdp_definition, f, indent=2)\n",
    "print(f\"MDP definition saved to mdp_definition_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77f9ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "### Model Used\n",
    "**NVIDIA Cosmos Reason 2B** - A vision-language model based on Qwen3-VL architecture, fine-tuned for physical world understanding and reasoning.\n",
    "\n",
    "### Part B: Object and Human Action Recognition\n",
    "- **B.1**: Object detection using natural language prompting to identify all objects in each video\n",
    "- **B.2**: Action recognition to identify manipulation actions (grasp, move, place, stack, etc.)\n",
    "\n",
    "### Part C: Automatic Generation of Sequence of Actions\n",
    "- **C.1**: Structured action sequence generation in JSON format with preconditions and effects\n",
    "- **C.2**: Markov Decision Process design with states, actions, transition probabilities, and rewards\n",
    "\n",
    "### Output Files Generated\n",
    "- `object_detection_results_*.json` - Object detection for all videos\n",
    "- `action_recognition_results_*.json` - Action recognition for all videos\n",
    "- `action_sequence_results_*.json` - Structured action sequences\n",
    "- `mdp_definition_*.json` - MDP formal definition\n",
    "- `mdp_diagram.png` - Visual diagram of the MDP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
